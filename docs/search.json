[
  {
    "objectID": "vis_cmri.html",
    "href": "vis_cmri.html",
    "title": "",
    "section": "",
    "text": "%cd ..\n\nc:\\Users\\s2558406\\Documents\\Repos\\cmr-experiments\n\n\nc:\\Users\\s2558406\\Documents\\Repos\\cmr-experiments\\venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n\n\n\nfrom utils.edipo.data.transforms import CineNetDataTransform\nfrom utils.edipo.data.mri_data import RawDataSample\nfrom utils.edipo.models.crnn import CRNN\nfrom utils import *\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader, random_split\nimport torch\nimport numpy as np\n\n\n# ONLY RUN when new dataset_cache run on Linux\nimport pathlib, pickle\nwith open(\"dataset_cache.pkl\", \"rb\") as f:\n    with set_posix_windows():\n        dataset_cache = pickle.load(f)\n    \n    new_cache = {\n        pathlib.WindowsPath(r\"M:\\data\\CMRxRecon\\SingleCoil\\Cine\\TrainingSet\"):\n    [\n        RawDataSample(r.fname.replace(\"/home/s2558406/RDS\", \"M:\").replace(\"/\", \"\\\\\"), r.slice_ind, r.metadata) for r in dataset_cache[\n            pathlib.WindowsPath('/home/s2558406/RDS/data/CMRxRecon/SingleCoil/Cine/TrainingSet')\n        ]\n    ]}\n\nwith open(\"dataset_cache_windows.pkl\", \"wb\") as f:\n    pickle.dump(new_cache, f)\n    \n\n\ndataset = DeepinvSliceDataset(\n    Path(r\"M:\\data\\CMRxRecon\"), \n    transform=CineNetDataTransform(time_window=12, apply_mask=True, normalize=False), \n    set_name=\"TrainingSet\",\n    acc_folders=[\"FullSample\"],\n    mask_folder=\"TimeVaryingGaussianMask16\",#\"AccFactor08\",\n    dataset_cache_file=\"dataset_cache_windows.pkl\"\n    )\n\nUsing dataset cache file\n\n\n\ntrain_dataset, test_dataset = random_split(dataset, (0.8, 0.2))\n\n\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=False)\ntest_dataloader  = DataLoader(dataset= test_dataset, batch_size=1, shuffle=False)\n\n\nx, y, mask = next(iter(train_dataloader))\n\n\nphysics = DynamicMRI((1, 2, 12, 512, 256))\ny2 = physics(x, mask=mask)\n\n\nx_hat = physics.A_adjoint(y2, mask=mask)\n\n\nmodel = ArtifactRemovalCRNN(\n    CRNN(num_cascades=2)\n).to(\"cpu\")\n\n\nphysics.set_mask(mask)\nx_recon = model(y, physics)\n\n\nplot_gif([x, y, y2, x_hat, mask], titles=[\"x\", \"y\", \"y2\", \"x_hat\", \"mask\"], display=True)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\nimport deepinv as dinv\n\n\nclass Undersampling(dinv.physics.DecomposablePhysics):\n    def __init__(self, mask, device=\"cpu\", **kwargs):\n        super().__init__(**kwargs)\n        self.mask = torch.nn.Parameter(mask.to(device), requires_grad=False)\n\n\nloss = YamanSplittingLoss(split_ratio=0.9)\n\nmask2 = loss.subsample_mask(loss.rng, physics.mask.data.detach().clone())\n\ninp = Undersampling(mask2, device=y.device)\n\ninp2 = Undersampling(mask - mask2, device=y.device)\n\nphysics1 = DynamicMRI(physics.img_size)\nphysics1.set_mask(mask2)\nphysics2 = DynamicMRI(physics.img_size)\nphysics2.set_mask(mask - mask2)\n\n# divide measurements\ny1 = inp.A(y)\ny2 = inp2.A(y)\n\n\nplot_videos([\n    mask, mask2, mask-mask2, \n    y, y1, y2, \n    physics.A_adjoint(y, mask=mask), physics1.A_adjoint(y1, mask=mask2), physics2.A_adjoint(y2, mask=mask-mask2)\n], display=True)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "demo_splitting_loss_tomography.html",
    "href": "demo_splitting_loss_tomography.html",
    "title": "Self-supervised learning with measurement splitting",
    "section": "",
    "text": "We demonstrate self-supervised learning with measurement splitting, to train a denoiser network on the MNIST dataset. The physics here is noisy computed tomography, as is the case in Noise2Inverse. Note this example can also be easily applied to undersampled multicoil MRI as is the case in SSDU.\nMeasurement splitting constructs a ground-truth free loss \\(\\frac{m}{m_2}\\| y_2 - A_2 \\inversef{y_1}{A_1}\\|^2\\) by splitting the measurement and the forward operator using a randomly generated mask.\nSee :class:deepinv.loss.SplittingLoss for full details.\nimport deepinv as dinv\nfrom torch.utils.data import DataLoader\nimport torch\nfrom torchvision import transforms, datasets\nfrom deepinv.models.utils import get_weights_url\n\nc:\\Users\\s2558406\\Documents\\Repos\\deepinv\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\ntorch.manual_seed(0)\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
  },
  {
    "objectID": "demo_splitting_loss_tomography.html#train-and-test-network",
    "href": "demo_splitting_loss_tomography.html#train-and-test-network",
    "title": "Self-supervised learning with measurement splitting",
    "section": "Train and test network",
    "text": "Train and test network\n\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    epochs=3,\n    losses=loss,\n    optimizer=optimizer,\n    device=device,\n    train_dataloader=train_dataloader,\n    plot_images=False,\n    save_path=None,\n    verbose=True,\n    show_progress_bar=False,\n    wandb_vis=False,\n)\n\nmodel = trainer.train()\n\nThe model has 444737 trainable parameters\nTrain epoch 0: TotalLoss=0.032, PSNR=29.155\nTrain epoch 1: TotalLoss=0.035, PSNR=28.895\nTrain epoch 2: TotalLoss=0.035, PSNR=28.837\n\n\nTest and visualise the model outputs using a small test set. We set the output to average over 5 iterations of random mask realisations. The trained model improves on the no-learning reconstruction by ~7dB.\n\ntrainer.plot_images = True\ntrainer.test(test_dataloader, pinv=True)\n\n\n\n\nTest PSNR: No learning rec.: 24.549+-1.052 | Model: 31.911+-2.220. \n\n\n(31.911066627502443, 2.219884675922773, 24.548791694641114, 1.0523162766060832)\n\n\nDemonstrate the effect of not averaging over multiple realisations of the splitting mask at evaluation time, by setting eval_n_samples=1. We have a worse performance:\n\nmodel.eval_n_samples = 1\ntrainer.test(test_dataloader, pinv=True)\n\n\n\n\nTest PSNR: No learning rec.: 24.549+-1.052 | Model: 30.434+-2.487. \n\n\n(30.434445762634276, 2.486670644991658, 24.548791694641114, 1.0523162766060832)\n\n\nFurthermore, we can disable measurement splitting at evaluation altogether by setting eval_split_input to False (this is done in SSDU). This generally is worse than MC averaging:\n\nmodel.eval_split_input = False\ntrainer.test(test_dataloader, pinv=True)\n\n\n\n\nTest PSNR: No learning rec.: 24.549+-1.052 | Model: 31.003+-2.107. \n\n\n(31.002875900268556, 2.106733038650352, 24.548791694641114, 1.0523162766060832)"
  },
  {
    "objectID": "demo_splitting_loss.html",
    "href": "demo_splitting_loss.html",
    "title": "Self-supervised learning with measurement splitting",
    "section": "",
    "text": "We demonstrate self-supervised learning with measurement splitting, to train a denoiser network on the MNIST dataset.\nMeasurement splitting constructs a ground-truth free loss \\(\\frac{m}{m_2}\\| y_2 - A_2 \\inversef{y_1}{A_1}\\|^2\\) by splitting the measurement and the forward operator using a randomly generated mask.\nSee :class:deepinv.loss.SplittingLoss for full details.\nimport deepinv as dinv\nfrom torch.utils.data import DataLoader\nimport torch\nfrom torchvision import transforms, datasets\nfrom deepinv.models.utils import get_weights_url\n\nc:\\Users\\s2558406\\Documents\\Repos\\deepinv\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\ntorch.manual_seed(0)\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
  },
  {
    "objectID": "demo_splitting_loss.html#train-and-test-network",
    "href": "demo_splitting_loss.html#train-and-test-network",
    "title": "Self-supervised learning with measurement splitting",
    "section": "Train and test network",
    "text": "Train and test network\n\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    epochs=1,\n    losses=loss,\n    optimizer=optimizer,\n    device=device,\n    train_dataloader=train_dataloader,\n    plot_images=False,\n    save_path=None,\n    verbose=True,\n    show_progress_bar=False,\n    wandb_vis=False,\n)\n\nmodel = trainer.train()\n\nThe model has 444737 trainable parameters\nTrain epoch 0: TotalLoss=0.017, PSNR=11.59\n\n\nTest and visualise the model outputs using a small test set. We set the output to average over 50 iterations of random mask realisations. The trained model improves on the no-learning reconstruction by ~3dB.\n\ntrainer.plot_images = True\nmodel.MC_samples = 50\ntrainer.test(test_dataloader)\n\n\n\n\nTest PSNR: No learning rec.: 19.356+-1.523 | Model: 23.150+-1.996. \n\n\n(23.150262069702148,\n 1.9961603806415074,\n 19.355849266052246,\n 1.5233685706910973)\n\n\nSince this is a denoising example, above, we have set eval_split_output to True (see :class:deepinv.loss.SplittingLoss for details). Alternatively, we get worse results when we set eval_split_output to False:\n\nmodel.eval_split_output = False\ntrainer.test(test_dataloader)\n\n\n\n\nTest PSNR: No learning rec.: 19.356+-1.523 | Model: 14.441+-1.520. \n\n\n(14.44115686416626, 1.5199918435073472, 19.355849266052246, 1.5233685706910973)\n\n\nFurthermore, we can disable measurement splitting at evaluation altogether by setting eval_split_input to False (this is done in SSDU):\n\nmodel.eval_split_input = False\ntrainer.test(test_dataloader)\n\n\n\n\nTest PSNR: No learning rec.: 19.356+-1.523 | Model: 9.650+-1.670. \n\n\n(9.650477170944214, 1.6700495788637173, 19.355849266052246, 1.5233685706910973)"
  },
  {
    "objectID": "demo_ei.html",
    "href": "demo_ei.html",
    "title": "Introduction to Equivariant Imaging",
    "section": "",
    "text": "Train a neural network to solve an image inpainting inverse problem, using perspective-EI and the deepinv library.\n\nimport deepinv as dinv\n\n\n\ntorch imports\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Compose, ToTensor, CenterCrop, Resize\nfrom torchvision.datasets.utils import download_and_extract_archive\n\n\nDefine inpainting experiment to reconstruct images from images masked with a random mask:\n\nphysics = dinv.physics.Inpainting((3, 256, 256), mask=0.6, device=\"cpu\")\n\nLoad Urban100 dataset of natural urban scenes:\n\n\nDownload dataset from HuggingFace\ndownload_and_extract_archive(\n    \"https://huggingface.co/datasets/eugenesiow/Urban100/resolve/main/data/Urban100_HR.tar.gz?download=true\",\n    \"Urban100\",\n    filename=\"Urban100_HR.tar.gz\",\n    md5=\"65d9d84a34b72c6f7ca1e26a12df1e4c\"\n)\n\n\n\ntrain_dataset, test_dataset = random_split(\n    ImageFolder(\"Urban100\", transform=Compose([ToTensor(), Resize(256), CenterCrop(256)])),\n    (0.8, 0.2)\n    )\n    \ntrain_dataloader, test_dataloader = DataLoader(train_dataset, shuffle=True), DataLoader(test_dataset)\n\nAs these scenes are imaged with a camera free to move and rotate in the world, we can impose perspective invariance on the unknown image set \\(x\\in X\\). Define measurement consistency and EI losses:\n\ntransform = dinv.transform.Homography(theta_max=10)\n\nlosses = [\n    dinv.loss.MCLoss(), \n    dinv.loss.EILoss(transform)\n]\n\nFor training, use a small UNet for the model with Adam optimizer:\n\nmodel = dinv.models.UNet(\n    in_channels=3, \n    out_channels=3,\n    scales=2,\n    circular_padding=True,\n    batch_norm=False\n)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n\nTrain the model using deepinv’s Trainer:\n\nmodel = dinv.Trainer(\n    model=model,\n    physics=physics,\n    online_measurements=True,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    epochs=1,\n    losses=losses,\n    optimizer=optimizer,\n    verbose=True,\n    show_progress_bar=False,\n    save_path=None,\n    device=\"cpu\"\n).train()\n\nThe model has 444867 trainable parameters\nEval epoch 0: PSNR=10.078\nTrain epoch 0: MCLoss=0.002, EILoss=0.021, TotalLoss=0.023, PSNR=15.948\n\n\nShow results of a pretrained model trained using a larger UNet for 40 epochs:\n\n\nLoad pretrained model from HuggingFace\nmodel = dinv.models.UNet(\n    in_channels=3, \n    out_channels=3,\n    scales=3,\n    circular_padding=True,\n    batch_norm=False\n)\n\nckpt = torch.hub.load_state_dict_from_url(\n    dinv.models.utils.get_weights_url(\"ei\", \"Urban100_inpainting_homography_model.pth\"),\n    map_location=\"cpu\",\n)\n\nmodel.load_state_dict(ckpt[\"state_dict\"])\n\n\n&lt;All keys matched successfully&gt;\n\n\n\nx, _ = next(iter(train_dataloader))\ny = physics(x)\nx_hat = model(y)\n\ndinv.utils.plot([x, y, x_hat], [\"x\", \"y\", \"reconstruction\"])"
  },
  {
    "objectID": "demo_splitting_loss_amri.html",
    "href": "demo_splitting_loss_amri.html",
    "title": "Self-supervised learning with measurement splitting",
    "section": "",
    "text": "We demonstrate self-supervised learning with measurement splitting, to train a denoiser network on the MNIST dataset. The physics here is noiseless undersampled MRI.\nimport deepinv as dinv\nfrom torch.utils.data import DataLoader\nimport torch\nfrom torchvision import transforms, datasets\nfrom deepinv.models.utils import get_weights_url\nfrom deepinv.physics.generator import GaussianMaskGenerator\nfrom numpy.random import default_rng\n\nc:\\Users\\s2558406\\Documents\\Repos\\deepinv\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\ntorch.manual_seed(0)\ndevice = dinv.utils.get_freer_gpu() if torch.cuda.is_available() else \"cpu\""
  },
  {
    "objectID": "demo_splitting_loss_amri.html#train-and-test-network",
    "href": "demo_splitting_loss_amri.html#train-and-test-network",
    "title": "Self-supervised learning with measurement splitting",
    "section": "Train and test network",
    "text": "Train and test network\n\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    epochs=20,#3\n    losses=loss,\n    optimizer=optimizer,\n    device=device,\n    train_dataloader=train_dataloader,\n    plot_images=False,\n    save_path=None,\n    verbose=True,\n    show_progress_bar=False,\n    wandb_vis=False,\n    rescale_mode=\"min_max\",\n)\n\nmodel = trainer.train()\n\nThe model has 445378 trainable parameters\nTrain epoch 0: TotalLoss=0.052, PSNR=11.962\nTrain epoch 1: TotalLoss=0.029, PSNR=12.593\nTrain epoch 2: TotalLoss=0.024, PSNR=12.697\nTrain epoch 3: TotalLoss=0.02, PSNR=12.912\nTrain epoch 4: TotalLoss=0.019, PSNR=13.057\nTrain epoch 5: TotalLoss=0.018, PSNR=13.029\nTrain epoch 6: TotalLoss=0.017, PSNR=13.17\nTrain epoch 7: TotalLoss=0.015, PSNR=13.207\nTrain epoch 8: TotalLoss=0.014, PSNR=13.245\nTrain epoch 9: TotalLoss=0.015, PSNR=13.316\nTrain epoch 10: TotalLoss=0.013, PSNR=13.334\nTrain epoch 11: TotalLoss=0.013, PSNR=13.371\nTrain epoch 12: TotalLoss=0.012, PSNR=13.448\nTrain epoch 13: TotalLoss=0.013, PSNR=13.38\nTrain epoch 14: TotalLoss=0.012, PSNR=13.527\nTrain epoch 15: TotalLoss=0.011, PSNR=13.559\nTrain epoch 16: TotalLoss=0.012, PSNR=13.477\nTrain epoch 17: TotalLoss=0.011, PSNR=13.589\nTrain epoch 18: TotalLoss=0.01, PSNR=13.65\nTrain epoch 19: TotalLoss=0.01, PSNR=13.709\n\n\n\ntorch.save({\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}, \"demo_measplit_mnist_amri.pth\")\n\nTest\n\ntrainer.plot_images = True\ntrainer.test(test_dataloader)\n\nThe model has 445378 trainable parameters\nEval epoch 0: PSNR=16.108, PSNR no learning=13.999\nTest results:\nPSNR no learning: 13.999 +- 1.112\nPSNR: 16.108 +- 2.293\n\n\n\n\n\n{'PSNR no learning': 13.99926462173462,\n 'PSNR no learning_std': 1.1116921880018424,\n 'PSNR': 16.108364486694335,\n 'PSNR_std': 2.293143492454918}\n\n\nDemonstrate the effect of not averaging over multiple realisations of the splitting mask at evaluation time, by setting eval_n_samples=1. We have a worse performance:\n\nmodel.eval_n_samples = 1\ntrainer.test(test_dataloader)\n\nThe model has 445378 trainable parameters\nEval epoch 0: PSNR=14.354, PSNR no learning=13.999\nTest results:\nPSNR no learning: 13.999 +- 1.112\nPSNR: 14.354 +- 2.984\n\n\n\n\n\n{'PSNR no learning': 13.99926462173462,\n 'PSNR no learning_std': 1.1116921880018424,\n 'PSNR': 14.353833389282226,\n 'PSNR_std': 2.9838297099206}\n\n\nFurthermore, we can disable measurement splitting at evaluation altogether by setting eval_split_input to False (this is done in SSDU). This generally is worse than MC averaging:\n\nmodel.eval_split_input = False\ntrainer.test(test_dataloader)\n\nThe model has 445378 trainable parameters\nEval epoch 0: PSNR=11.126, PSNR no learning=13.999\nTest results:\nPSNR no learning: 13.999 +- 1.112\nPSNR: 11.126 +- 2.130\n\n\n\n\n\n{'PSNR no learning': 13.99926462173462,\n 'PSNR no learning_std': 1.1116921880018424,\n 'PSNR': 11.126238059997558,\n 'PSNR_std': 2.12953882578676}"
  },
  {
    "objectID": "demo_tour_mri.html",
    "href": "demo_tour_mri.html",
    "title": "Tour of MRI functionality in DeepInverse",
    "section": "",
    "text": "This example presents the various datasets, forward physics and models available in DeepInverse for Magnetic Resonance Imaging (MRI) problems:\n\nPhysics: :class:deepinv.physics.MRI, :class:deepinv.physics.MultiCoilMRI, :class:deepinv.physics.DynamicMRI\nDatasets: the full FastMRI dataset :class:deepinv.datasets.FastMRISliceDataset and a lightweight, easy-to-use subset :class:deepinv.datasets.SimpleFastMRISliceDataset\nModels: :class:deepinv.models.VarNet (VarNet/E2E-VarNet), :class:deepinv.utils.demo.demo_mri_model (a simple MoDL unrolled model)\n\nContents: 1. Get started with FastMRI (singlecoil + multicoil) 2. Train an accelerated MRI with neural networks 3. Load raw FastMRI data (singlecoil + multicoil) 4. Train using raw data 5. Explore 3D MRI 6. Explore dynamic MRI\n\nimport deepinv as dinv\nimport torch, torchvision\n\ndevice = dinv.utils.get_freer_gpu if torch.cuda.is_available() else \"cpu\"\nrng = torch.Generator(device=device).manual_seed(0)\n\nc:\\Users\\s2558406\\Documents\\Repos\\deepinv\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n1. Get started with FastMRI\nYou can get started with our simple FastMRI slice subsets which provide quick, easy-to-use, in-memory datasets which can be used for simulation experiments.\n.. important::\nBy using this dataset, you confirm that you have agreed to and signed the `FastMRI data use agreement &lt;https://fastmri.med.nyu.edu/&gt;`_.\nLoad knee and brain datasets (original data is 320x320 but we resize to 128 for speed):\n\ntransform = torchvision.transforms.Resize(128)\nknee_dataset = dinv.datasets.SimpleFastMRISliceDataset(\n    dinv.utils.get_data_home(), anatomy=\"knee\", transform=transform, train=True, download=True\n)\nbrain_dataset = dinv.datasets.SimpleFastMRISliceDataset(\n    dinv.utils.get_data_home(), anatomy=\"brain\", transform=transform, train=True, download=True\n)\n\n\nimg_size = knee_dataset[0].shape[-2:] # (128, 128)\ndinv.utils.plot({\n    \"knee\": knee_dataset[0],\n    \"brain\": brain_dataset[0]\n})\n\n\n\n\nDefine single-coil MRI physics. We can define a constant Cartesian 4x undersampling mask by sampling once from a physics generator. The mask, data and measurements will all be of shape (B, C, H, W) where C=2 is the real and imaginary parts.\n\nphysics_generator = dinv.physics.generator.GaussianMaskGenerator(img_size=img_size, acceleration=4, rng=rng, device=device)\nmask = physics_generator.step()[\"mask\"]\n\nphysics = dinv.physics.MRI(mask=mask, img_size=img_size, device=device)\n\ndinv.utils.plot({\n    \"x\": (x := knee_dataset[0].unsqueeze(0)),\n    \"mask\": mask,\n    \"y\": physics(x).clamp(-1, 1)\n})\nprint(\"Shapes:\", x.shape, physics.mask.shape)\n\n\n\n\nShapes: torch.Size([1, 2, 128, 128]) torch.Size([1, 2, 128, 128])\n\n\nGenerate an accelerated single-coil MRI measurement dataset. Use knees for training and brains for testing.\nWe can also use the physics generator to randomly sample a new mask per sample, and save the masks alongside the measurements:\n\ndataset_path = dinv.datasets.generate_dataset(\n    train_dataset=knee_dataset,\n    test_dataset=brain_dataset,\n    val_dataset=None,\n    physics=physics,\n    physics_generator=physics_generator,\n    save_physics_generator_params=True,\n    overwrite_existing=False,\n    device=device,\n    save_dir=dinv.utils.get_data_home(),\n    batch_size=1,\n)\n\ntrain_dataset = dinv.datasets.HDF5Dataset(dataset_path, split=\"train\", load_physics_generator_params=True)\ntest_dataset  = dinv.datasets.HDF5Dataset(dataset_path, split=\"test\",  load_physics_generator_params=True)\n\ndinv.utils.plot({\n    \"x0\":    train_dataset[0][0],\n    \"mask0\": train_dataset[0][2][\"mask\"],\n    \"x1\":    train_dataset[1][0],\n    \"mask1\": train_dataset[1][2][\"mask\"],\n})\n\nC:\\Users\\s2558406\\Documents\\Repos\\deepinv\\deepinv\\datasets\\datagenerator.py:220: UserWarning: Dataset datasets/dinv_dataset0.h5 already exists, skipping...\n  warn(f\"Dataset {hf_path} already exists, skipping...\")\n\n\n\n\n\nYou can also simulate multicoil MRI data. Either pass in ground-truth coil maps, or pass an integer to simulate simple birdcage coil maps. The measurements y are now of shape (B, C, N, H, W), where N is the coil-dimension.\n\nmc_physics = dinv.physics.MultiCoilMRI(img_size=img_size, coil_maps=3, device=device)\n\ndinv.utils.plot({\n    \"x\": x,\n    \"coil_map_0\": mc_physics.coil_maps.abs()[:, 0, ...],\n    \"coil_map_1\": mc_physics.coil_maps.abs()[:, 1, ...],\n    \"coil_map_2\": mc_physics.coil_maps.abs()[:, 2, ...],\n    \"x RSS\": mc_physics.A_adjoint_A(x, mask=mask, rss=True)\n})\n\n\n\n\n\n\n2. Train an accelerated MRI problem with neural networks\nTrain a neural network to solve the inverse problem. We provide various models specifically used for MRI reconstruction. These are unrolled networks which require a denoiser, such as UNet or DnCNN as a backbone:\n\ndenoiser = dinv.models.UNet(\n    in_channels=2,\n    out_channels=2,\n    scales=2,\n)\n\ndenoiser = dinv.models.DnCNN(\n    in_channels=2,\n    out_channels=2,\n    pretrained=None,\n    depth=2,\n)\n\nWe provide some specific implementations, including VarNet/E2E-VarNet and MoDL:\n\nmodel = dinv.models.VarNet(denoiser, num_cascades=2, mode=\"varnet\").to(device)\n\nmodel = dinv.utils.demo.demo_mri_model(denoiser, num_iter=2, device=device).to(device)\n\nTrain a network with supervised or self-supervised (using Equivariant Imaging) loss.\nFor speed, we only use a very small 2-layer DnCNN inside a unrolled network with 2 cascades, and train with 5 images for 1 epoch, but load a pretrained model that has been trained with 10 images for 50 epochs:\n\nloss = dinv.loss.SupLoss()\nloss = dinv.loss.EILoss(transform=dinv.transform.CPABDiffeomorphism())\n\ntrainer = dinv.Trainer(\n    model=model,\n    physics=physics,\n    optimizer=torch.optim.Adam(model.parameters()),\n    train_dataloader=(train_dataloader := torch.utils.data.DataLoader(torch.utils.data.Subset(train_dataset, range(5)))),\n    epochs=1,\n    show_progress_bar=False,\n    save_path=None\n)\n\nurl = dinv.models.utils.get_weights_url(model_name=\"demo\", file_name=\"demo_tour_mri.pth\")\nckpt = torch.hub.load_state_dict_from_url(url, map_location=lambda storage, loc: storage, file_name=\"demo_tour_mri.pth\")\ntrainer.model.load_state_dict(ckpt[\"state_dict\"])\ntrainer.optimizer.load_state_dict(ckpt[\"optimizer\"])\n\nmodel = trainer.train()\ntrainer.plot_images = True\n\nThe model has 2376 trainable parameters\nTrain epoch 0: TotalLoss=0.001, PSNR=30.263\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\nTest the model: we improve the PSNR compared to the zero-filled reconstruction, both on the train (knee) set and the test (brain) set:\n\n_ = trainer.test(train_dataloader)\n\nEval epoch 0: PSNR=30.295, PSNR no learning=27.169\nTest results:\nPSNR no learning: 27.169 +- 1.845\nPSNR: 30.295 +- 1.207\n\n\n\n\n\n\n_ = trainer.test(torch.utils.data.DataLoader(torch.utils.data.Subset(test_dataset, range(5))))\n\nEval epoch 0: PSNR=30.229, PSNR no learning=28.409\nTest results:\nPSNR no learning: 28.409 +- 1.409\nPSNR: 30.229 +- 1.556\n\n\n\n\n\n\n\n3. Load raw FastMRI data\nThe raw multi-coil FastMRI data is provided as pairs of (x, y) where y are the fully-sampled k-space measurements of arbitrary size, and x are the cropped root-sum-square (RSS) magnitude reconstructions.\n\n#dinv.datasets.download_archive(dinv.utils.get_image_url(\"fastmri_brain_multicoil_train_0.h5\"), dinv.utils.get_data_home() / \"brain\" / \"fastmri.h5\")\n\ndataset = dinv.datasets.FastMRISliceDataset(dinv.utils.get_data_home() / \"brain\", slice_index=\"middle\")\n\nx, y = dataset[0]\n\nx, y = x.unsqueeze(0), y.unsqueeze(0)\n\nprint(\"Shapes:\", x.shape, y.shape) # x (B, 1, W, W); y (B, C, N, H, W)\n\nimg_shape, kspace_shape = x.shape[-2:], y.shape[-2:]\nn_coils = y.shape[2]\n\n100%|██████████| 1/1 [00:00&lt;?, ?it/s]\n\n\nShapes: torch.Size([1, 1, 320, 320]) torch.Size([1, 2, 20, 640, 320])\n\n\nWe can relate x and y using our :class:deepinv.physics.MultiCoilMRI (note that since we aren’t provided with the ground-truth coil-maps, we can only perform the adjoint operator).\n\nphysics = dinv.physics.MultiCoilMRI(\n    img_size=img_shape,\n    mask=torch.ones(kspace_shape),\n    coil_maps=torch.ones((n_coils,) + kspace_shape, dtype=torch.complex64),\n    device=device\n)\n\nx_rss = physics.A_adjoint(y, rss=True, crop=True)\n\nassert torch.allclose(x, x_rss)\n\n\n\n4. Train using raw data\nWe now use a mask generator to generate acceleration masks on-the-fly (“online”) during training. We use the E2E-VarNet model designed for multicoil MRI. We do not perform coil sensitivity map estimation and simply assume they are flat as above. To do this yourself, pass a model as the sensitivity_model parameter.\n\nphysics_generator = dinv.physics.generator.GaussianMaskGenerator(img_size=kspace_shape, acceleration=4, rng=rng, device=device)\n\n\nmodel = dinv.models.VarNet(denoiser, num_cascades=2, mode=\"e2e-varnet\").to(device)\n\nNote that we require overriding the base :class:deepinv.training.Trainer to deal with raw measurements, as we do not want to generate k-space measurements, only mask it.\n.. note ::\nWe require `loop_physics_generator=True` and `shuffle=False` in the dataloader to ensure that each image is always matched with the same random mask at each iteration.\n\nclass RawFastMRITrainer(dinv.Trainer):\n    def get_samples_online(self, iterators, g):\n        # Get data\n        x, y = next(iterators[g])\n        x, y = x.to(self.device), y.to(self.device)\n\n        # Get physics\n        physics = self.physics[g]\n\n        # Generate random mask\n        params = self.physics_generator[g].step(batch_size=y.size(0), img_size=y.shape[-2:])\n\n        # Generate measurements directly from raw measurements\n        y *= params[\"mask\"]\n        \n        physics.update_parameters(**params)\n\n        return x, y, physics\n\nWe also need to modify the metrics used to crop the model output when comparing to the cropped magnitude RSS targets:\n\nclass CropMSE(torch.nn.MSELoss):\n    def forward(self, input, target):\n        transform = torchvision.transforms.CenterCrop(target.shape[-2:])\n        return super().forward(dinv.metric.functional.complex_abs(transform(input)), target)\n\nclass CropPSNR(dinv.loss.PSNR):\n    def forward(self, x_net = None, x = None, *args, **kwargs):\n        transform = torchvision.transforms.CenterCrop(x.shape[-2:])\n        return super().forward(transform(x_net), x, *args, **kwargs)\n\n\ntrainer = RawFastMRITrainer(\n    model=model,\n    physics=physics,\n    physics_generator=physics_generator,\n    online_measurements=True,\n    loop_physics_generator=True,\n    losses=dinv.loss.SupLoss(metric=CropMSE()),\n    metrics=CropPSNR(),\n    optimizer=torch.optim.Adam(model.parameters()),\n    train_dataloader=torch.utils.data.DataLoader(dataset, shuffle=False),\n    epochs=1,\n    save_path=None,\n    show_progress_bar=False,\n)\n_ = trainer.train()\n\nC:\\Users\\s2558406\\Documents\\Repos\\deepinv\\deepinv\\training\\trainer.py:192: UserWarning: Generated measurements repeat each epoch. Ensure that dataloader is not shuffling.\n  warnings.warn(\nc:\\Users\\s2558406\\Documents\\Repos\\deepinv\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 1, 320, 320])) that is different to the input size (torch.Size([1, 1, 20, 320, 320])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n\n\nThe model has 2372 trainable parameters\nTrain epoch 0: TotalLoss=0.783, CropPSNR=4.072\n\n\n\n\n5. Explore 3D MRI\nWe use a demo 3D brain volume of shape (181, 217, 181) and simulate 3D single-coil or multi-coil Fourier measurements using :class:deepinv.physics.MRI or :class:deepinv.physics.MultiCoilMRI.\n\nx = torch.from_numpy(dinv.utils.demo.load_np_url(\n    \"https://huggingface.co/datasets/deepinv/images/resolve/main/brainweb_t1_ICBM_1mm_subject_0.npy?download=true\"\n)).unsqueeze(0).unsqueeze(0).to(device)\nx = torch.cat([x, torch.zeros_like(x)], dim=1) # add imaginary dimension\n\nprint(x.shape) # (B, C, D, H, W) where D is depth\n\ntorch.Size([1, 2, 181, 217, 181])\n\n\n\nphysics = dinv.physics.MultiCoilMRI(img_size=x.shape[1:], three_d=True, device=device)\nphysics = dinv.physics.MRI(img_size=x.shape[1:], three_d=True, device=device)\n\n\ndinv.utils.plot_ortho3D([x, physics(x)], titles=[\"x\", \"y\"])\n\n\n\n\n\n\n6. Explore dynamic MRI\nFinally, we show how to use the dynamic MRI for image sequence data of shape (B, C, T, H, W) where T is the time dimension. note that this is also compatible with 3D MRI. We simulate an MRI image sequence using the first 5 knees:\n\nx = torch.stack([knee_dataset[i] for i in range(5)], dim=1).unsqueeze(0)\n\nGenerate a Cartesian k-t sampling mask and simulate k-t-space measurements:\n\nphysics_generator = dinv.physics.generator.EquispacedMaskGenerator(img_size=img_size, acceleration=4, rng=rng, device=device)\nmask = physics_generator.step()[\"mask\"]\nphysics = dinv.physics.DynamicMRI(mask=mask, img_size=img_size, device=device)\n\ny = physics(x)\n\nprint(x.shape, physics(x).shape)\n\ntorch.Size([1, 2, 5, 128, 128]) torch.Size([1, 2, 5, 128, 128])"
  }
]