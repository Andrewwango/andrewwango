[
  {
    "objectID": "demo_quick_ei.html",
    "href": "demo_quick_ei.html",
    "title": "Introduction to Equivariant Imaging",
    "section": "",
    "text": "Train a neural network to solve an image inpainting inverse problem, using perspective-EI and the deepinv library.\n\nimport deepinv as dinv\n\n\n\ntorch imports\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Compose, ToTensor, CenterCrop, Resize\nfrom torchvision.datasets.utils import download_and_extract_archive\n\n\nDefine inpainting experiment to reconstruct images from images masked with a random mask:\n\nphysics = dinv.physics.Inpainting((3, 256, 256), mask=0.6, device=\"cpu\")\n\nDownload and load Urban100 dataset of natural urban scenes:\n\ndownload_and_extract_archive(\n    \"https://huggingface.co/datasets/eugenesiow/Urban100/resolve/main/data/Urban100_HR.tar.gz?download=true\",\n    \"Urban100\",\n    filename=\"Urban100_HR.tar.gz\",\n    md5=\"65d9d84a34b72c6f7ca1e26a12df1e4c\"\n)\n\nDownloading https://cdn-lfs.huggingface.co/datasets/eugenesiow/Urban100/25b929945e053de19bf8f575bd0821abab1eac7f5f6c5c7980221d8ed13066c6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Urban100_HR.tar.gz%3B+filename%3D%22Urban100_HR.tar.gz%22%3B&response-content-type=application%2Fgzip&Expires=1714649844&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDY0OTg0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9ldWdlbmVzaW93L1VyYmFuMTAwLzI1YjkyOTk0NWUwNTNkZTE5YmY4ZjU3NWJkMDgyMWFiYWIxZWFjN2Y1ZjZjNWM3OTgwMjIxZDhlZDEzMDY2YzY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=t8c0WHbGVQtHUqobyTjdSIFuyeVqGSSQmEbBvVCnXer303wnh986fWQ6addRajsQ1KJ%7EC6gFFpgNHQIKQe4METS3g0%7EFqGn9%7Efo7E3mvLC9E6f%7EI8RMoqYiWFjyg9y0yG4JLHwuuZlr-wXs6OrsFxSOod0mrA610GCYQfSJUwR5RnGPA1A1YA237k-bQgwPHuEjzIVjhsUTQnAsBrQco525WqLKOXaweSiKWcYvYR-UHcWl37RW%7E61BRNuJXfv3wHX--vS-n3-KT3ptRIuOOP2e94ZdTWsAb4xfE3UiGG9ITi024RpR%7EhZKmtzvFpes-1Rcte5rxqXC2uucawHoPjA__&Key-Pair-Id=KVTP0A1DKRTAX to Urban100\\Urban100_HR.tar.gz\nExtracting Urban100\\Urban100_HR.tar.gz to Urban100\n\n\n100%|██████████| 135388067/135388067 [00:10&lt;00:00, 12603864.84it/s]\n\n\n\ntrain_dataset, test_dataset = random_split(\n    ImageFolder(\"Urban100\", transform=Compose([ToTensor(), Resize(256), CenterCrop(256)])),\n    (0.8, 0.2)\n    )\n    \ntrain_dataloader, test_dataloader = DataLoader(train_dataset, shuffle=True), DataLoader(test_dataset)\n\nAs these scenes are imaged with a camera free to move and rotate in the world, we can impose perspective invariance on the unknown image set \\(x\\in X\\).\nFor training, use a small UNet, Adam optimizer, EI loss with homography transform, and the deepinv.Trainer functionality:\n\nmodel = dinv.models.UNet(\n    in_channels=3, \n    out_channels=3,\n    scales=2,\n    circular_padding=True,\n    batch_norm=False\n)\n\nlosses = [\n    dinv.loss.MCLoss(), \n    dinv.loss.EILoss(dinv.transform.Homography(theta_max=10))\n]\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)\n\nTrain the model using deepinv’s Trainer:\n\nmodel = dinv.Trainer(\n    model=model,\n    physics=physics,\n    online_measurements=True,\n    train_dataloader=train_dataloader,\n    eval_dataloader=test_dataloader,\n    epochs=1,\n    losses=losses,\n    optimizer=optimizer,\n    verbose=True,\n    show_progress_bar=False,\n    save_path=None,\n    device=\"cpu\"\n).train()\n\nThe model has 444867 trainable parameters\nEval epoch 0: PSNR=10.078\nTrain epoch 0: MCLoss=0.002, EILoss=0.021, TotalLoss=0.023, PSNR=15.948\n\n\nShow results of a pretrained model trained using a larger UNet for 40 epochs:\n\nmodel = dinv.models.UNet(\n    in_channels=3, \n    out_channels=3,\n    scales=3,\n    circular_padding=True,\n    batch_norm=False\n)\n\nckpt = torch.hub.load_state_dict_from_url(\n    dinv.models.utils.get_weights_url(\"ei\", \"Urban100_inpainting_homography_model.pth\"),\n    map_location=\"cpu\",\n)\n\nmodel.load_state_dict(ckpt[\"state_dict\"])\n\n&lt;All keys matched successfully&gt;\n\n\n\nx, _ = next(iter(train_dataloader))\ny = physics(x)\nx_hat = model(y)\n\ndinv.utils.plot([x, y, x_hat], [\"x\", \"y\", \"reconstruction\"])"
  }
]